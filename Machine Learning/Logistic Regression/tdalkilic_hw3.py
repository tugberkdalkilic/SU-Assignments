# -*- coding: utf-8 -*-
"""Copy of HW3 - Logistic Regression - toSolve.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-2UwALx-9ZAFyoR_4z5hyeLiEEMf5102
"""

import matplotlib.pyplot as plt
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import numpy as np

"""# [25 pts] A Toy Example: Decision Boundary and Conditional Independence Assumption

### Gaussian Distributed Data fits better to Gaussian Naive Bayesian rather than Logistic Regression, unfortunately that is not the case most of the time.
Now, imagine we have two artificial dataset. Both are drawn from Gaussian distribution. One of the dataset is with standard deviation 1 and the other is 5. Each cluster is conditionally independent from each other.

make_blobs function samples data points from gaussian distribution.
"""

from sklearn.datasets import make_blobs
data1, label1 = make_blobs(n_samples=500, centers=2, n_features=2, random_state=1, cluster_std=1)
data2, label2 = make_blobs(n_samples=500, centers=2, n_features=2, random_state=1, cluster_std=5)

"""Let's split the datasets into train and test."""

x1_train, x1_test, y1_train, y1_test= train_test_split(data1,label1,test_size=0.2, random_state=15)
x2_train, x2_test, y2_train, y2_test= train_test_split(data2,label2,test_size=0.2, random_state=15)

"""Plot the first dataset with standard deviation 1."""

plt.scatter(data1[:,0], data1[:,1])
plt.title('Scatter plot data with standard deviation=1')
plt.xlabel('x')
plt.ylabel('y')
plt.show()

"""Plot the first dataset with standard deviation 4."""

plt.scatter(data2[:,0], data2[:,1])
plt.title('Scatter plot data with standard deviation=5')
plt.xlabel('x')
plt.ylabel('y')
plt.show()

"""Train a Gaussian Naive Bayesian and Logistic Regression with the 1st dataset."""

bayes= GaussianNB()
bayes.fit(x1_train,y1_train)

x_min, x_max = x1_train[:,0].min() - .5, x1_train[:,0].max() + .5
y_min, y_max = x1_train[:,1].min() - .5, x1_train[:,1].max() + .5
h = .02
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
Z = bayes.predict(np.c_[xx.ravel(), yy.ravel()])

# Put the result into a color plot
Z = Z.reshape(xx.shape)
plt.figure(1, figsize=(4, 3))
plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)

# Plot also the training points
plt.scatter(x1_train[:, 0], x1_train[:, 1], c=y1_train, edgecolors='k', cmap=plt.cm.Paired)
plt.xlabel('X -- Standard deviation 1')
plt.ylabel('y')

plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.xticks(())
plt.yticks(())

plt.show()

logist= LogisticRegression(random_state=0)
logist.fit(x1_train,y1_train)

x_min, x_max = x1_train[:,0].min() - .5, x1_train[:,0].max() + .5
y_min, y_max = x1_train[:,1].min() - .5, x1_train[:,1].max() + .5
h = .02
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
Z = logist.predict(np.c_[xx.ravel(), yy.ravel()])

# Put the result into a color plot
Z = Z.reshape(xx.shape)
plt.figure(1, figsize=(4, 3))
plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)

# Plot also the training points
plt.scatter(x1_train[:, 0], x1_train[:, 1], c=y1_train, edgecolors='k', cmap=plt.cm.Paired)
plt.xlabel('x')
plt.ylabel('y')

plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.xticks(())
plt.yticks(())

plt.show()

"""### Perfect Decision Boundary"""

predict1= bayes.predict(x1_test)
print("Classification Report for Naive Bayesian:")
print(classification_report(y1_test,predict1))

confusion_matrix(y1_test, predict1)

print(accuracy_score(y1_test, predict1))

predict2= logist.predict(x1_test)
print("Classification Report for Logistic Regression:")
print(classification_report(y1_test,predict2))

print(accuracy_score(y1_test, predict2))

confusion_matrix(y1_test, predict2)

"""### Both algorithm perfectly separate two data clusters for 1st dataset with standard deviation 1. The data points are linearly separable."""

bayes2= GaussianNB()
bayes2.fit(x2_train, y2_train)

x_min, x_max = x2_train[:,0].min() - .5, x2_train[:,0].max() + .5
y_min, y_max = x2_train[:,1].min() - .5, x2_train[:,1].max() + .5
h = .02
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
Z = bayes2.predict(np.c_[xx.ravel(), yy.ravel()])

# Put the result into a color plot
Z = Z.reshape(xx.shape)
plt.figure(1, figsize=(4, 3))
plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)

# Plot also the training points
plt.scatter(x2_train[:, 0], x2_train[:, 1], c=y2_train, edgecolors='k', cmap=plt.cm.Paired)
plt.xlabel('X-- standard deviation 5')
plt.ylabel('Y')

plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.xticks(())
plt.yticks(())

plt.show()

logist2= LogisticRegression(C=10)
logist2.fit(x2_train,y2_train)

x_min, x_max = x2_train[:,0].min() - .5, x2_train[:,0].max() + .5
y_min, y_max = x2_train[:,1].min() - .5, x2_train[:,1].max() + .5
h = .02
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
Z = logist2.predict(np.c_[xx.ravel(), yy.ravel()])

# Put the result into a color plot
Z = Z.reshape(xx.shape)
plt.figure(1, figsize=(4, 3))
plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)

# Plot also the training points
plt.scatter(x2_train[:, 0], x2_train[:, 1], c=y2_train, edgecolors='k', cmap=plt.cm.Paired)
plt.xlabel('x')
plt.ylabel('y')

plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.xticks(())
plt.yticks(())

plt.show()

predict3=bayes2.predict(x2_test)
print("Classification Report for Naive Bayesian:")
print(classification_report(y2_test,predict3))

print(accuracy_score(y2_test, predict3))

confusion_matrix(y2_test, predict3)

predict4=logist2.predict(x2_test)
print("Classification Report for Logistic Regression:")
print(classification_report(y2_test,predict4))

confusion_matrix(y2_test, predict4)

print(accuracy_score(y2_test, predict4))

"""### Use the scatter plot and draw the perfect decision boundary on two scatter plot. Discuss what is linear separability, decision boundary, which datapoints are harder to separate. Discuss the accuries and the why which model performs better.
 

### Please also read: [Equivalence of GNB and LR](https://appliedmachinelearning.blog/2019/09/30/equivalence-of-gaussian-naive-bayes-and-logistic-regression-an-explanation/)

# [75pts] Logistic Regression and Naive Bayesian Comparison

### The dataset
We will use Kaggle dataset. This dataset contains around 200k news headlines from the year 2012 to 2018 obtained from HuffPost.

You can [download.](https://www.kaggle.com/rmisra/news-category-dataset)
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer

df=pd.read_json("drive/My Drive/Colab Notebooks/CS412_hw3/News_Category_Dataset_v2.json", lines=True)

"""## Select 4 categories: Politics, Wellness, Entertainment, Travel

use only 50K of data row
"""

df = df.sample(50000)

new_df = df[(df['category']== 'POLITICS') | (df['category']== 'WELLNESS') | (df['category']== 'ENTERTAINMENT') | (df['category']== 'TRAVEL')]

new_df['category'].value_counts()

"""Convert category names to digit labelling"""

y = (new_df['category'].to_numpy() == "WELLNESS")*1 + (new_df['category'].to_numpy() == "ENTERTAINMENT")*2 + (new_df['category'].to_numpy() == "TRAVEL")*3

"""Merge headlines with short descriptions"""

X = new_df['short_description'] + ' '+ new_df['headline']

"""###Â Create Tf-Idf model"""

from sklearn.feature_extraction.text import CountVectorizer

count_vect = CountVectorizer(lowercase=True, stop_words='english')
X_train_counts = count_vect.fit_transform(X)

from sklearn.feature_extraction.text import TfidfTransformer

tfidf_transformer = TfidfTransformer()
X_train_tf = tfidf_transformer.fit_transform(X_train_counts)

"""Split train and test data"""

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test= train_test_split(X_train_tf,y,test_size=0.2)

X_train_tf.shape

"""### Gaussian Naive Bayesian"""

GNB = GaussianNB(var_smoothing=0.2)
GNB.fit(x_train.toarray(),y_train)

predict5= GNB.predict(x_test.toarray())
print("Classification Report for Naive Bayesian:")
print(classification_report(y_test,predict5))

print(accuracy_score(y_test, predict5))

print(confusion_matrix(y_test, predict5))

"""### 6) Logistic Regression"""

LR= LogisticRegression(random_state=15, max_iter=1000, C=10)
LR.fit(x_train.toarray(),y_train)

predict6= LR.predict(x_test.toarray())
print("Classification Report for Logistic Regression:")
print(classification_report(predict6,y_test))

print(accuracy_score(y_test, predict6))

print(confusion_matrix(y_test, predict6))

"""### Observe Logistic Regression is much slower but more accurate. Discuss."""